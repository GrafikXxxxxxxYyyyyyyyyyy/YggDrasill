{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoisePredictor model has successfully loaded from 'GrafikXxxxxxxYyyyyyyyyyy/sdxl_Juggernaut' checkpoint!\n",
      "VAE model has successfully loaded from 'GrafikXxxxxxxYyyyyyyyyyy/sdxl_Juggernaut' checkpoint!\n",
      "TextEncoder model has successfully loaded from 'GrafikXxxxxxxYyyyyyyyyyy/sdxl_Juggernaut' checkpoint!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from YggDrasill.stable_diffusion_model import StableDiffusionModel, StableDiffusionModelKey\n",
    "\n",
    "\n",
    "KEY = StableDiffusionModelKey(\n",
    "    device = \"cuda\",\n",
    "    model_type = \"sdxl\",\n",
    "    dtype = torch.float16,\n",
    "    is_latent_model = True,\n",
    "    scheduler_name = \"euler\",\n",
    "    model_path = \"GrafikXxxxxxxYyyyyyyyyyy/sdxl_Juggernaut\",\n",
    ")\n",
    "\n",
    "sd = StableDiffusionModel(**KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from YggDrasill.stable_diffusion_pipeline import (\n",
    "    StableDiffusionPipeline,\n",
    "    DiffusionPipelineInput,\n",
    "    TextEncoderPipelineInput,\n",
    "    StableDiffusionPipelineInput,\n",
    ")\n",
    "from YggDrasill.core.pipelines.forward_diffusion import ForwardDiffusionInput\n",
    "\n",
    "\n",
    "sd_input = StableDiffusionPipelineInput(\n",
    "    diffusion_input=DiffusionPipelineInput(\n",
    "        forward_input=ForwardDiffusionInput(\n",
    "            num_inference_steps=50,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    te_input=TextEncoderPipelineInput(\n",
    "        prompt = [\n",
    "            \"Cat\",\n",
    "            \"DOG\"\n",
    "        ],\n",
    "        # prompt_2 = [\n",
    "        #     \"\"\n",
    "        # ],\n",
    "        negative_prompt = [\n",
    "            \"\",\n",
    "            \"\"\n",
    "        ],\n",
    "        # negative_prompt_2 = [\n",
    "        #     \"\",\n",
    "        # ],\n",
    "        clip_skip = 2,\n",
    "        lora_scale = None,\n",
    "        num_images_per_prompt=4,\n",
    "    ),  \n",
    "\n",
    "    use_refiner=False,\n",
    "    guidance_scale=7.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionPipeline --->\n",
      "TextEncoderPipeline --->\n",
      "CLIPTextEncoderPipeline --->\n",
      "CLIPTextEncoderModel --->\n",
      "torch.Size([8, 77, 768])\n",
      "CLIPTextEncoderPipeline --->\n",
      "CLIPTextEncoderModel --->\n",
      "torch.Size([16, 77, 768])\n",
      "StableDiffusionModel --->\n",
      "DiffusionPipeline --->\n",
      "DiffusionModel --->\n",
      "Scheduler has successfully changed to 'euler'\n",
      "Scheduler has successfully changed to 'euler'\n",
      "ForwardDiffusion --->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDiffusion --->\n",
      "\n",
      "\n",
      "\n",
      "Conditions(class_labels=None, prompt_embeds=tensor([[[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.0681,  0.1464, -0.1630,  ...,  0.2783, -0.4246,  0.2632],\n",
      "         [-0.0410,  0.1710, -0.2418,  ...,  0.0843,  0.1886, -0.1136],\n",
      "         ...,\n",
      "         [ 0.1730, -0.2250, -0.0512,  ..., -0.1731,  0.4377,  0.8511],\n",
      "         [ 0.1785, -0.2263, -0.0416,  ..., -0.2230,  0.3535,  0.8979],\n",
      "         [ 0.1984, -0.1827, -0.0088,  ..., -0.1914,  0.5732,  0.8838]],\n",
      "\n",
      "        [[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.0681,  0.1464, -0.1630,  ...,  0.2783, -0.4246,  0.2632],\n",
      "         [-0.0410,  0.1710, -0.2418,  ...,  0.0843,  0.1886, -0.1136],\n",
      "         ...,\n",
      "         [ 0.1730, -0.2250, -0.0512,  ..., -0.1731,  0.4377,  0.8511],\n",
      "         [ 0.1785, -0.2263, -0.0416,  ..., -0.2230,  0.3535,  0.8979],\n",
      "         [ 0.1984, -0.1827, -0.0088,  ..., -0.1914,  0.5732,  0.8838]],\n",
      "\n",
      "        [[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.0681,  0.1464, -0.1630,  ...,  0.2783, -0.4246,  0.2632],\n",
      "         [-0.0410,  0.1710, -0.2418,  ...,  0.0843,  0.1886, -0.1136],\n",
      "         ...,\n",
      "         [ 0.1730, -0.2250, -0.0512,  ..., -0.1731,  0.4377,  0.8511],\n",
      "         [ 0.1785, -0.2263, -0.0416,  ..., -0.2230,  0.3535,  0.8979],\n",
      "         [ 0.1984, -0.1827, -0.0088,  ..., -0.1914,  0.5732,  0.8838]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.7100,  0.3486,  0.1613,  ...,  0.0224,  0.4968, -0.1406],\n",
      "         [-0.0662,  0.3740,  0.0181,  ...,  0.1121, -0.1212, -0.1658],\n",
      "         ...,\n",
      "         [ 0.2073, -0.1763, -0.0198,  ..., -0.0647,  0.5283,  0.7739],\n",
      "         [ 0.2146, -0.1758, -0.0128,  ..., -0.1149,  0.4434,  0.8188],\n",
      "         [ 0.2319, -0.1342,  0.0160,  ..., -0.0517,  0.6572,  0.8213]],\n",
      "\n",
      "        [[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.7100,  0.3486,  0.1613,  ...,  0.0224,  0.4968, -0.1406],\n",
      "         [-0.0662,  0.3740,  0.0181,  ...,  0.1121, -0.1212, -0.1658],\n",
      "         ...,\n",
      "         [ 0.2073, -0.1763, -0.0198,  ..., -0.0647,  0.5283,  0.7739],\n",
      "         [ 0.2146, -0.1758, -0.0128,  ..., -0.1149,  0.4434,  0.8188],\n",
      "         [ 0.2319, -0.1342,  0.0160,  ..., -0.0517,  0.6572,  0.8213]],\n",
      "\n",
      "        [[-3.0312, -1.9863,  3.7832,  ...,  0.0214,  0.1011, -0.1583],\n",
      "         [-0.7100,  0.3486,  0.1613,  ...,  0.0224,  0.4968, -0.1406],\n",
      "         [-0.0662,  0.3740,  0.0181,  ...,  0.1121, -0.1212, -0.1658],\n",
      "         ...,\n",
      "         [ 0.2073, -0.1763, -0.0198,  ..., -0.0647,  0.5283,  0.7739],\n",
      "         [ 0.2146, -0.1758, -0.0128,  ..., -0.1149,  0.4434,  0.8188],\n",
      "         [ 0.2319, -0.1342,  0.0160,  ..., -0.0517,  0.6572,  0.8213]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>), timestep_cond=None, attention_mask=None, cross_attention_kwargs=None, added_cond_kwargs={'text_embeds': tensor([[-0.4822,  0.7949, -0.7646,  ..., -1.0801, -1.0850,  1.0967],\n",
      "        [-0.4822,  0.7949, -0.7646,  ..., -1.0801, -1.0850,  1.0967],\n",
      "        [-0.4822,  0.7949, -0.7646,  ..., -1.0801, -1.0850,  1.0967],\n",
      "        ...,\n",
      "        [-0.9287,  0.5742,  0.4092,  ..., -1.9043, -1.1191, -0.2710],\n",
      "        [-0.9287,  0.5742,  0.4092,  ..., -1.9043, -1.1191, -0.2710],\n",
      "        [-0.9287,  0.5742,  0.4092,  ..., -1.9043, -1.1191, -0.2710]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>), 'time_ids': tensor([[1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.],\n",
      "        [1024., 1024.,    0.,    0., 1024., 1024.]], device='cuda:0',\n",
      "       dtype=torch.float16)})\n",
      "tensor(981., device='cuda:0')\n",
      "torch.Size([16, 4, 1024, 1024])\n",
      "torch.Size([16, 77, 2048])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 1280])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 7.52 GiB is free. Process 2848123 has 8.22 GiB memory in use. Process 2859042 has 7.91 GiB memory in use. Of the allocated memory 7.25 GiB is allocated by PyTorch, and 209.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43msd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msd_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/YggDrasill/stable_diffusion_pipeline.py:80\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, model, diffusion_input, te_input, use_refiner, guidance_scale, refiner_steps, refiner_scale, aesthetic_score, negative_aesthetic_score, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Учитывая переданные аргументы, используем полученный/ые пайплайны\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     diffusion_pipeline \u001b[38;5;241m=\u001b[39m DiffusionPipeline()\n\u001b[0;32m---> 80\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffuser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffuser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdiffusion_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StableDiffusionPipelineOutput(\n\u001b[1;32m     90\u001b[0m     images\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/dev/YggDrasill/core/diffusion_pipeline.py:116\u001b[0m, in \u001b[0;36mDiffusionPipeline.__call__\u001b[0;34m(self, diffuser, width, height, conditions, image, generator, mask_image, forward_input, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(forward_output\u001b[38;5;241m.\u001b[39mtimesteps)):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# TODO: Добавить расширение условий за счёт ControlNet\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# <...>\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     backward_output\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m--> 116\u001b[0m     backward_output \u001b[38;5;241m=\u001b[39m \u001b[43mBACKWARD\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffuser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackward_output\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# TODO: Добавить обработку маски через image\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# в случае если модель не для inpainting\u001b[39;00m\n\u001b[1;32m    126\u001b[0m vae_output \u001b[38;5;241m=\u001b[39m IMAGE_PROCESSOR(\n\u001b[1;32m    127\u001b[0m     vae\u001b[38;5;241m=\u001b[39mdiffuser\u001b[38;5;241m.\u001b[39mvae,\n\u001b[1;32m    128\u001b[0m     latents\u001b[38;5;241m=\u001b[39mbackward_output\u001b[38;5;241m.\u001b[39mnoisy_sample\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/dev/YggDrasill/core/pipelines/backward_diffusion.py:62\u001b[0m, in \u001b[0;36mBackwardDiffusion.__call__\u001b[0;34m(self, predictor, timestep, noisy_sample, conditions, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([model_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_sample, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_sample], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)   \n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Получаем предсказание шума\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m noise_predict \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Учитываем CFG\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_cfg:\n",
      "File \u001b[0;32m~/dev/YggDrasill/core/models/noise_predictor.py:170\u001b[0m, in \u001b[0;36mNoisePredictor.__call__\u001b[0;34m(self, timestep, noisy_sample, conditions, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(extra_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_cond_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(extra_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_cond_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 170\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_noise\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1169\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1164\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_encoder_hidden_states(\n\u001b[1;32m   1165\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states, added_cond_kwargs\u001b[38;5;241m=\u001b[39madded_cond_kwargs\n\u001b[1;32m   1166\u001b[0m )\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# 2. pre-process\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# 2.5 GLIGEN position net\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 0 has a total capacity of 23.65 GiB of which 7.52 GiB is free. Process 2848123 has 8.22 GiB memory in use. Process 2859042 has 7.91 GiB memory in use. Of the allocated memory 7.25 GiB is allocated by PyTorch, and 209.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "StableDiffusionPipeline()(sd, **sd_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
